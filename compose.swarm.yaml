# ============================================================================
# Docker Swarm - Compose for Production Cluster
# ============================================================================
# This file is optimized for Docker Swarm and consolidates:
# - Overlay networks for communication between nodes
# - Native Swarm configs (automatic distribution of configuration files)
# - Native Swarm secrets (secure credential management)
# - Deploy with replicas, updates, and rollbacks
# - Placement constraints to control where services run
# - Swarm-specific healthchecks and restart policies
# - Resource limits and structured logging
# 
# IMPORTANT: This file requires Docker Swarm mode to be activated
# 
# IMPLEMENTED BEST PRACTICES:
# ✓ Configs for automatic distribution of configuration files
# ✓ Encrypted secrets for sensitive data
# ✓ Isolated overlay networks (frontend/backend)
# ✓ Healthchecks for failure detection
# ✓ Rolling updates with zero downtime
# ✓ Automatic rollback in case of failure
# ✓ Resource limits for predictability
# ✓ Structured logging with rotation
# ✓ Security options (no-new-privileges, cap_drop)
# ============================================================================

# ============================================================================
# NETWORK CONFIGURATION
# ============================================================================
# Overlay networks allow communication between containers on different Swarm nodes
# Automatically created with IPsec encryption between nodes
networks:
  # Frontend network: exposed to the outside world via nginx
  frontend:
    driver: overlay  # Required for Swarm - allows multi-node communication
    ipam:
      config:
        - subnet: 172.20.0.0/24
    # Labels for documentation
    labels:
      com.docker.compose.network: "frontend"
      description: "Public network for nginx and external access"

  # Backend network: isolated, internal communication only
  backend:
    driver: overlay  # Required for Swarm - allows multi-node communication
    ipam:
      config:
        - subnet: 172.21.0.0/24
    # Total isolation: no external access, only app and database communicate
    internal: true  # Blocks external access - containers only communicate internally
    labels:
      com.docker.compose.network: "backend"
      description: "Isolated private network for app and database"

# ============================================================================
# VOLUME CONFIGURATION
# ============================================================================
volumes:
  # ============================================================================
  # Volume for PostgreSQL data
  # ============================================================================
  # IMPORTANT: In production, storage choice is critical for database performance,
  # availability, and backup.
  #
  # OPTION 1: Native Docker Volume (CURRENT - Recommended for single-node)
  # Better performance, managed by Docker, adequate isolation
  postgres_data:
    driver: local
    # Docker automatically manages under /var/lib/docker/volumes/
    # Advantages:
    # - Better performance (optimized by Docker)
    # - Automatic permission management
    # - Integration with backup via plugins
    # - Portability between environments
    labels:
      description: "Persistent PostgreSQL data"
      backup: "daily"
      critical: "true"
  
  # OPTION 2: Bind Mount with absolute path
  # Use when you need full control over the storage location
  # postgres_data:
  #   driver: local
  #   driver_opts:
  #     type: none
  #     o: bind
  #     device: /var/lib/docker-volumes/postgres  # ABSOLUTE Path
  #   # Advantages:
  #   # - Full control of the location (e.g., mount on dedicated SSD/NVMe)
  #   # - Facilitates automated backup with external scripts
  #   # - Useful for compliance (auditing physical data location)
  #   labels:
  #     description: "Persistent PostgreSQL data"
  
  # OPTION 3: NFS - Network File System (Ideal for clusters/high availability)
  # Use in distributed environments where multiple nodes need to access the data
  # file_storage:
  #   driver: local
  #   driver_opts:
  #     type: nfs
  #     o: addr=192.168.1.100,rw,nfsvers=4.1,hard,intr
  #     device: ":/mnt/nfs/file_storage"
  #   # Advantages:
  #   # - Sharing between multiple nodes
  #   # - Facilitates failover and high availability
  #   # - Centralized backup on the storage server
  #   # - Horizontal scalability
  #   # Disadvantages:
  #   # - Network latency (impact on performance)
  #   # - Dependency on the NFS server (single point of failure)
  #   # - Requires additional security configuration
  #   labels:
  #     description: "File storage via NFS"

  # OPTION 4: Block Storage (AWS EBS, DigitalOcean Volumes, GCP Persistent Disks)
  # Use in cloud environments with dedicated managed volumes for high performance
  # postgres_data:
  #   driver: local
  #   driver_opts:
  #     type: none
  #     o: bind
  #     device: /mnt/block-storage/postgres  # External mounted volume
  #   # PREREQUISITE: Mount the block storage volume on the host first:
  #   # sudo mkdir -p /mnt/block-storage
  #   # sudo mount /dev/disk/by-id/scsi-0DO_Volume_postgres-vol /mnt/block-storage
  #   # 
  #   # Advantages:
  #   # - Guaranteed performance (dedicated IOPS)
  #   # - Automatic snapshots for backup
  #   # - Dynamic size expansion
  #   # - Migration between instances (attach/detach)
  #   # - Provider SLA for availability
  #   # Disadvantages:
  #   # - Additional costs per GB/IOPS
  #   # - Limited to one availability zone
  #   # - Requires configuration in the cloud provider
  #   labels:
  #     description: "Persistent PostgreSQL data via Block Storage"

  # Volume for application logs
  # PRODUCTION: Docker managed volume (more secure and portable)
  app_logs:
    driver: local
    labels:
      description: "Spring application logs"

# ============================================================================
# CONFIGS
# ============================================================================
# Native Docker Swarm Configs - automatically distributed to all nodes
# Ideal for non-sensitive configuration files (nginx.conf, etc)
# 
# IMPORTANT: Create configs BEFORE deploying:
#   docker config create nginx_conf ./nginx/nginx.conf
#   docker config create nginx_default_conf ./nginx/conf.d/default.conf
# 
# Check created configs:
#   docker config ls
#   docker config inspect nginx_conf
#
# TROUBLESHOOTING:
# - If the service doesn't start, check if the configs exist (docker config ls)
# - Configs are immutable: to update, create a new config and update the service
# - Use 'docker service ps <service> --no-trunc' to see config-related errors
configs:
  nginx_conf:
    external: true  # Config already exists in Swarm
  nginx_default_conf:
    external: true  # Config already exists in Swarm

# ============================================================================
# SECRETS
# ============================================================================
# Native Docker Swarm Secrets - stored encrypted and securely distributed
# between nodes. Mounted at /run/secrets/ inside containers.
# 
# IMPORTANT: Create secrets BEFORE deploying:
#   echo "your_secure_password" | docker secret create db_password -
#   echo "db_user" | docker secret create db_user -
#   echo "secret_jwt_token" | docker secret create jwt_secret -
# 
# Check created secrets:
#   docker secret ls
#   docker secret inspect db_password
secrets:
  db_password:
    external: true  # Secret already exists in Swarm
  db_user:
    external: true  # Secret already exists in Swarm

# ============================================================================
# CONFIGURATIONS
# ============================================================================
# .env file is automatically loaded
# Variables can be replaced using ${VARIABLE:-default}

# ============================================================================
# SERVICES
# ============================================================================
services:
  
  # ============================================================================
  # NGINX - Reverse Proxy
  # ============================================================================
  nginx:
    image: nginx:1.29-alpine
    
    # Networks: connected to frontend (external) and backend (app)
    networks:
      - frontend
      - backend
    
    # Exposed ports
    ports:
      - 80:80
      # - target: 80
      #   published: 80
      #   protocol: tcp
      #   mode: ingress  # Swarm routing mesh - automatic load balance between replicas
    
    # Configs - Automatically distributed by Swarm to all nodes
    configs:
      - source: nginx_conf
        target: /etc/nginx/nginx.conf
        mode: 0444  # read-only
      - source: nginx_default_conf
        target: /etc/nginx/conf.d/default.conf
        mode: 0444  # read-only
    
    # Dependencies - NOTE: in Swarm, depends_on only controls start order, doesn't guarantee ready state
    depends_on:
      - app
    
    # Healthcheck
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s  # Nginx initializes very quickly
    
    # Swarm deploy configuration
    deploy:
      replicas: 3  # 3 instances - 1 per node for uniform distribution
      
      # Resource limits and reservations per replica
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
      
      # Swarm restart policy
      restart_policy:
        condition: on-failure  # Restart only on failure
        delay: 5s
        max_attempts: 3
        window: 30s  # If runs for 30s without failing, considers success and resets counter
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,tier,environment"
        tag: "nginx"
    
    # Labels for organization
    labels:
      service: "nginx"
      tier: "frontend"
      environment: "${ENVIRONMENT:-production}"
    
  # ==========================================================================
  # SPRING-BOOT APPLICATION
  # ==========================================================================
  app:
    image: mcqueide/people-api:v1.0.0

    # Networks: backend (app, db) and frontend (nginx)
    networks:
      - backend
      - frontend
    
    environment:
      - SPRING_PROFILES_ACTIVE=docker
      - SPRING_DATASOURCE_URL=jdbc:postgresql://postgres:5432/people
      - SPRING_DATASOURCE_USERNAME=people
      - SPRING_DATASOURCE_PASSWORD=people

    # Secrets
    secrets:
      - db_password
      - db_user

    # Volumes
    volumes:
      - app_logs:/app/logs
    
    # Dependencies - NOTE: in Swarm, depends_on only controls start order, doesn't guarantee healthcheck
    depends_on:
      - postgres
    
    # Swarm deploy and orchestration configuration
    deploy:
      # Resource limits and reservations per replica
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.5'
          memory: 256M
      
      replicas: 3  # 3 instances for load balancing
      
      # Update strategy: rolling update with zero downtime
      update_config:
        parallelism: 1      # Updates 1 replica at a time
        delay: 10s          # Waits 10s between updates
        order: start-first  # Starts new version before stopping the old one (zero downtime)
        failure_action: rollback  # Automatic rollback in case of update failure
      
      # Rollback strategy in case of update failure
      rollback_config:
        parallelism: 1  # Rollback 1 replica at a time
        delay: 5s       # Waits 5s between rollbacks
      
      
      # Swarm restart policy
      restart_policy:
        condition: on-failure  # Restarts only if container fails (exit code != 0)
        delay: 10s             # Waits 10s before restarting
        max_attempts: 15       # Maximum of 15 restart attempts
        window: 60s            # If runs for 60s without failing, considers success and resets counter
    
    # Structured logging
    logging:
      driver: "json-file"
      options:
        max-size: "50m"
        max-file: "5"
        labels: "service,tier,environment"
        tag: "{{.Name}}/{{.ID}}"
    
    # Labels
    labels:
      service: "app"
      tier: "backend"
      environment: "${ENVIRONMENT:-production}"
      version: "${APP_VERSION:-1.0.0}"
    
    # Drop all capabilities (principle of least privilege)
    cap_drop:
      - ALL
    
    # User namespace (security best practice)
    # user: "spring:spring"  # Uncomment if the image supports it

  # ==========================================================================
  # POSTGRESQL - Database
  # ==========================================================================
  postgres:
    image: postgres:16-alpine
    
    # Isolated backend network
    networks:
      - backend
    
    # Ports: DO NOT expose externally in production
    # ports:
    #   - "5432:5432"  # ❌ For debug only, remove in production
    
    environment:
      - POSTGRES_DB=people
      - POSTGRES_USER_FILE=/run/secrets/db_user
      - POSTGRES_PASSWORD_FILE=/run/secrets/db_password
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
      - PGDATA=/var/lib/postgresql/data/pgdata

    # Secrets
    secrets:
      - db_password
      - db_user
    
    # Persistent volumes
    volumes:
      - postgres_data:/var/lib/postgresql/data
    
    # OPTIMIZATION: Use tmpfs for temporary data (improves performance)
    # Sockets and temporary files do not need persistence
    tmpfs:
      - /tmp
      - /var/run/postgresql  # Unix Socket in memory for better performance
    
    # Healthcheck
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$(cat /run/secrets/db_user) -d ${DB_NAME:-appdb}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s  # PostgreSQL takes ~30s to initialize
    
    # Swarm deploy configuration
    deploy:
      replicas: 1  # Only 1 replica due to local volume
      
      # Resource limits and reservations
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '1.0'
          memory: 512M
      
      # Placement constraint: fixes PostgreSQL to a specific node
      # IMPORTANT: Required because the volume is "driver: local"
      # The volume only exists on the specific node, so the container must always run there
      # 
      # To use this constraint, first create the label on the node:
      #   docker node update --label-add database=true <node-name>
      placement:
        constraints:
          - node.labels.database == true  # Only runs on nodes with this custom label
      
      # Swarm restart policy
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 60s  # If runs for 60s without failing, considers success and resets counter
    
    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
        tag: "postgres"
    
    # Labels
    labels:
      service: "postgres"
      tier: "database"
    
    # Shared memory size (important for PostgreSQL)
    # NOTE: In Docker Swarm, shm_size is not directly supported
    # Alternative: use tmpfs mount or increase via sysctls on the host
    # shm_size: 256mb
    
# ============================================================================
# SWARM INITIALIZATION
# ============================================================================
# 
# 1. Initialize Swarm (if not already active):
#   docker swarm init
#
# 2. Create secrets before deployment:
#   echo "secure_password" | docker secret create db_password -
#   echo "db_user" | docker secret create db_user -
#   echo "jwt_token" | docker secret create jwt_secret -
#
# 3. Create configs before deployment:
#   docker config create nginx_conf ./nginx/nginx.conf
#   docker config create nginx_default_conf ./nginx/conf.d/default.conf
#
# 4. Create label on the node for PostgreSQL:
#   docker node update --label-add database=true <node-name>
#
# 5. Deploy the complete stack:
#   docker stack deploy -c compose.prod.swarm.yaml myapp
#
# 6. Check stack status:
#   docker stack ls
#   docker stack ps myapp
#   docker service ls
#
# 7. (Optional) Configure centralized logging:
#   For production, consider using:
#   - ELK Stack (Elasticsearch, Logstash, Kibana)
#   - Fluentd + Elasticsearch
#   - Loki + Grafana
#   - CloudWatch (AWS), Stackdriver (GCP), etc
#
# ============================================================================

# ============================================================================
# USEFUL SWARM COMMANDS
# ============================================================================
#
# === STACK MANAGEMENT ===
#
# Deploy/update stack:
#   docker stack deploy -c compose.prod.swarm.yaml myapp
#
# Remove complete stack:
#   docker stack rm myapp
#
# List stacks:
#   docker stack ls
#
# View stack services:
#   docker stack services myapp
#
# View stack tasks (containers):
#   docker stack ps myapp
#
# === SERVICE MANAGEMENT ===
#
# List services:
#   docker service ls
#
# Inspect service:
#   docker service inspect myapp_app
#
# View logs of a service (all replicas):
#   docker service logs -f myapp_app
#   docker service logs --tail=100 myapp_postgres
#
# Manually scale service:
#   docker service scale myapp_app=5
#
# Force service update (re-deploy):
#   docker service update --force myapp_app
#
# Rollback to previous version:
#   docker service rollback myapp_app
#
# View service replicas:
#   docker service ps myapp_app
#
# === NODE MANAGEMENT ===
#
# List Swarm nodes:
#   docker node ls
#
# Inspect node:
#   docker node inspect <node-name>
#
# Add label to node:
#   docker node update --label-add database=true <node-name>
#
# Remove label from node:
#   docker node update --label-rm database <node-name>
#
# === SECRETS MANAGEMENT ===
#
# List secrets:
#   docker secret ls
#
# Create secret:
#   echo "value" | docker secret create secret_name -
#
# Inspect secret (does not show the value):
#   docker secret inspect db_password
#
# Remove secret (must remove services using it first):
#   docker secret rm db_password
#
# === CONFIGS MANAGEMENT ===
#
# List configs:
#   docker config ls
#
# Create config from file:
#   docker config create nginx_conf ./nginx/nginx.conf
#
# Inspect config (shows metadata, not content):
#   docker config inspect nginx_conf
#
# View config content (base64 decode):
#   docker config inspect --pretty nginx_conf
#
# Remove config (must remove services using it first):
#   docker config rm nginx_conf
#
# Update config (configs are immutable, need to create new one and update service):
#   docker config create nginx_conf_v2 ./nginx/nginx.conf
#   docker service update --config-rm nginx_conf --config-add source=nginx_conf_v2,target=/etc/nginx/nginx.conf myapp_nginx
#
# === DATABASE OPERATIONS ===
#
# Execute command in specific container:
#   docker exec -it $(docker ps -q -f name=myapp_postgres) psql -U postgres -d appdb
#
# Database backup:
#   docker exec $(docker ps -q -f name=myapp_postgres) pg_dump -U postgres appdb > backup.sql
#
# Database restore:
#   docker exec -i $(docker ps -q -f name=myapp_postgres) psql -U postgres appdb < backup.sql
#
# === VOLUMES ===
#
# List volumes:
#   docker volume ls
#
# Inspect volume:
#   docker volume inspect myapp_postgres_data
#
# Volume backup (execute on the node where the volume is located):
#   docker run --rm -v myapp_postgres_data:/data -v $(pwd):/backup alpine tar czf /backup/postgres_backup.tar.gz -C /data .
#
# === MONITORING ===
#
# View resources used by services:
#   docker stats
#
# View Swarm events in real-time:
#   docker events
#
# View logs of all service replicas:
#   docker service logs -f myapp_app
#
# View logs of a specific container:
#   docker logs -f <container-id>
#
# === TROUBLESHOOTING ===
#
# Check why replicas are not starting:
#   docker service ps myapp_nginx --no-trunc
#
# View full error details:
#   docker inspect <container-id>
#
# Test connectivity between services:
#   docker exec -it <container-id> ping postgres
#   docker exec -it <container-id> nslookup postgres
#
# Check healthcheck of a container:
#   docker inspect --format='{{json .State.Health}}' <container-id> | jq
#
# Force replica redistribution:
#   docker service update --force myapp_app
#
# View update/rollback history:
#   docker service inspect --pretty myapp_app
#
# ============================================================================